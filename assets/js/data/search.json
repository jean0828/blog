[ { "title": "LDAP Monitoring with AMA and Microsoft Sentinel", "url": "/blog/posts/LDAP-Monitoring-with-AMA-and-Microsoft-Sentinel/", "categories": "siem, Active Directory", "tags": "active directory, siem, sentinel, sentinel", "date": "2023-08-23 13:00:00 +0000", "snippet": "According to Microsoft. LDAP is “an application protocol for working with various directory services. Directory services, such as Active Directory, store user and account information, and security ...", "content": "According to Microsoft. LDAP is “an application protocol for working with various directory services. Directory services, such as Active Directory, store user and account information, and security information like passwords. The service then allows the information to be shared with other devices on the network. Enterprise applications such as email, customer relationship managers (CRMs), and Human Resources (HR) software can use LDAP to authenticate, access, and find information.”.This Protocol provides clients the ability to open a simple bind to an Active Directory domain controller in order to establish an authenticated session. However, the major problem with this simple bind is that the initial credentials from the client to the domain controller are sent in clear text and that could allow a man-in-the-middle attacker to successfully forward an authentication request to a Windows LADP server that hasn’t been configured to required signing or sealing on incoming connections.Therefore, it’s important to monitor those events in our SIEM.This tutorial is based on this article petri.com.Enabling LDAP events in Domain ControllersTo begin with, LDAP logging can be set on domain controllers to help you identify where insecure LDAP bind attempts are coming from. we can do it by Powershell or Regedit. Using Powershell:New-ItemProperty -Path 'HKLM:\\SYSTEM\\CurrentControlSet\\Services\\NTDS\\Diagnostics' -Name \"16 LDAP Interface Events\" -Value 2 -PropertyType DWORD -Force Using RegeditGoing to HKLM\\SYSTEM\\CurrentControlSet\\Services\\NTDS\\Diagnostics and setting the value of the registry called 16 LDAP Interface Events to 2.NOTE: In case, you need a verbose logging to review a security event, it should set ‘15 Field Engineering ‘ to ‘5’.Events of Interest Event ID 2886: it indicates that LDAP signing is not being enforced by your domain controller. Event ID 2887: This event is logged each time a client computer attempts an unsigned LDAP bind. Event ID 2889: The event is logged whenever a client makes an LDAP bind that this directory Server is not configured to reject.Sending Logs To SentinelNow, it’s turn to install and selecting the events to ingest in Microsoft Sentinel. To do this task, it’s needed the following steps: Install AMA (Azure Monitor Agent) in the DC and select the events of interest to ingest.First, we have to go to Azure Monitor → Data Collection Rule → Create Data Collection RuleIn Resources, we add the DC to monitor and the log analytics workspace where the logs will be ingested Create the Xpath query to ingest events of interest.In collect and deliver tab, we have to go to custom and then add the specific XPath query to ingest LDAP events in the Log Analytics Workspace. For that reason, it’s mandatory to use the event viewer to know how the Xpath query should be.Finally, the XPath query should look similar to this:'Directory Service'!*[System[(EventID=2889 or EventID=2887 or EventID=2886)]] Testing the events were being sent to the log analytics workspaceFirst, we have to do a simple LDAP bind. For this case, the bind connection was made from the same controller using ldp tool.After some minutes, it’s possible to see those logs in the Log analytics Workspace.As we can see, LDAP monitoring enabled. Now, it’s possible to use the Workbook called Insecure Protocols to visualize insecure LDAP connections.Finally, after identifying the clients or applications that are using insecure binds, the next step is to make adjustments. Ensure that these clients or apps either use a secure TLS channel for basic LDAP binds." }, { "title": "Adding the Docker Build and Publish Stage in a Pipeline", "url": "/blog/posts/Adding-the-Docker-Build-and-Publish-Stage-in-a-Pipeline/", "categories": "devsecops", "tags": "kubernetes, google cloud, devsecops, jenkins, docker", "date": "2023-04-05 02:20:00 +0000", "snippet": "This is part from the course of Linux Foundation called Implementing DevSecOps and it’s given by initcron.In a development environment, you can build an image with Docker. However, in a Continuous ...", "content": "This is part from the course of Linux Foundation called Implementing DevSecOps and it’s given by initcron.In a development environment, you can build an image with Docker. However, in a Continuous Integration environment such as Jenkins, it’s not prudent to use Docker daemon, as it would need privileged access. You can however use tools such as Kaniko to build an image in a secure, non-privileged environment. Follow the steps in this section to achieve that.The step-by-step process to set up an automated container image build and publish process involves the following: Setting up the credentials to connect to the container registry. Kaniko will read these credentials while being launched as part of a pipeline run by Jenkins. Adding a build agent configuration so that Jenkins knows which container image to use and how to launch a container/pod to run the job with Kaniko. Adding a stage to the Jenkins pipeline to launch Kaniko to build an image using Dockerfile in the source repository and publish it to the registry.Adding the Registry CredentialsKaniko is able to read the credentials store as Kubernetes secrets. Using Docker Hub as the container registry, we can create a secret with the registry credentials:kubectl create secret -n ci docker-registry regcred --docker-server=https://index.docker.io/v1/ --docker-username=user --docker-password=pass --docker-email=user@email.orgWe can check the secret was added with the commandkubectl get secrets -n ciCreate the Jenkins Agent ConfigurationAdd the build-agent configuration that Jenkins will use to create a container and run the image build job with it. This configuration uses Kaniko, which is an image build tool sitting inside Kubernetes and creates an image without requiring a Docker daemon. This is a secure alternative to using DIND, which is a privileged container, or mounting a Docker socket directly on the Jenkins host.we should edit the build-agent.yml file which is part of the project to add the Kaniko agent configuration, as in:apiVersion: v1kind: Podmetadata: labels: app: spring-build-cispec: containers: - name: maven image: maven:alpine command: - cat tty: true volumeMounts: - name: m2 mountPath: /root/.m2/ - name: docker-tools image: rmkanda/docker-tools:latest command: - cat tty: true volumeMounts: - mountPath: /var/run name: docker-sock - mountPath: /tmp/trivycache/ name: trivycache - name: kaniko\t image: gcr.io/kaniko-project/executor:v1.6.0-debug\t imagePullPolicy: Always\t command: \t - sleep\t args:\t - 99d\t volumeMounts:\t - name: jenkins-docker-cfg\t mountPath: /kaniko/.docker - name: trufflehog image: rmkanda/trufflehog command: - cat tty: true - name: licensefinder image: licensefinder/license_finder command: - cat tty: true volumes: - name: m2 hostPath: path: /tmp/.m2/ - name: docker-sock hostPath: path: /var/run - name: trivycache hostPath: path: /tmp/trivycache/\t- name: jenkins-docker-cfg\t projected:\t sources:\t - secret: \t name: regcred\t\t items:\t\t - key: .dockerconfigjson\t\t path: config.jsonAlso, we should add the stage to Jenkinsfile to build an image with Kaniko. When launched, this stage will use the registry credentials set up earlier, read the Dockerfile which is available as part of the source code repo, build an image and publish it to the registry.stage('Docker BnP') {\tsteps {\t\tcontainer('kaniko') {\t\t sh '/kaniko/executor -f `pwd`/Dockerfile -c `pwd` --insecure --skip-tls-verify --cache=true --destination=docker.io/xxxxxx/dsodemo'\t\t } \t} }In my case, the Jenkinsfile is:pipeline {  agent {    kubernetes {      yamlFile 'build-agent.yaml'      defaultContainer 'maven'      idleMinutes 1    }  }  stages {    stage('Build') {      parallel {        stage('Compile') {          steps {            container('maven') {              sh 'mvn compile'            }          }        }      }    }    stage('Test') {      parallel {        stage('Unit Tests') {          steps {            container('maven') {              sh 'mvn test'            }          }        }      }    }    stage('Package') {      parallel {        stage('Create Jarfile') {          steps {            container('maven') {              sh 'mvn package -DskipTests'            }          }        }        stage('Docker BnP') {          steps {            container('kaniko') {              sh '/kaniko/executor -- force -f `pwd`/Dockerfile -c `pwd` --insecure --skip-tls-verify --cache=true --destination=docker.io/jean0828/dsodemo'            }          }        }      }    }     stage('Deploy to Dev') {      steps {        // TODO        sh \"echo done\"      }    }  }}The project is located here. The pipeline will be execute when there is a new commit or when you add the pipeline as described in the first part. Then, when it’s completed, we obtain the following image:As we can see, a new step was added to the Package stage and it’s the step to build and publish the image in Docker Hub. Besides, we can confirm that was executed without problems because we can see the image in Docker Hub:Now, with simple steps we have a continuos integration pipeline." }, { "title": "Deploying a Cloud Environment for DevSecops", "url": "/blog/posts/Deploying-a-Cloud-Environment-for-DevSecops/", "categories": "devsecops", "tags": "kubernetes, google cloud, devsecops, jenkins", "date": "2023-02-28 03:20:00 +0000", "snippet": "For this case, we are going to use Google Cloud. There, a Kubernetes cluster will be created, Linux environment for development will be also created, and Jenkins will be deployed within Kubernetes....", "content": "For this case, we are going to use Google Cloud. There, a Kubernetes cluster will be created, Linux environment for development will be also created, and Jenkins will be deployed within Kubernetes.This is part from the course of Linux Foundation called Implementing DevSecOps and it’s given by initcron.Create a Kubernetes Cluster with GKEFirst, it’s recommended to create a new project where the environment will be deployed. To do that, we just have to click in New Project.For this case, the name is DevSecOps.After creating the project, we should enable the Kubernetes Engine API. To enable it, we have to go to navegation menu → Kubernetes Engine → Clusters → Enablewe have to wait some minutes to have Kubernetes Engine Api activated.When the Kubernetes, we can create the cluster with the following options. Cluster mode: Standard Release Channel: Rapid Channel Version: select the newestAs a result, we’ll have a Kubernetes cluster with 3 nodes.Next, it’s recommended to create a firewall rule to allow connections from your public IP. to crteate the rule, we can do it from Navigation Menu → VPC Network → Firewall.Then, we have to select the one which has the following naming convention: gke-cluster-X-XXXXX-all and add your IP.Now it’s time to connecto to our cluster and the easiest way to do it, it’s using the cloud shell and execute the command suggested.in this environment, this is the command:gcloud container clusters get-credentials cluster-1 --zone us-central1-c --project devsecops-378701After that, we can check the status of the cluster:kubectl get nodeskubectl config get-contextskubectl get pods --all-namespacesFinally, check that we also have helm installed with the command helm version.Creating a Linux Development EnvironmentThe Linux VM is where our development environment will be. Here, we can test our apps with dockers before to put it in the deployment environment.To create the Linux VM, we have to go to VM Instances and click in create instance wih those options: Size Disk: 30 GB Operating System: Ubuntu LTS Create or Import SSH Keys network tag: dev Startup Script: include this script:#!/bin/bashapt-get updateapt-get install -y git wget# Install Dockerapt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ software-properties-commoncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -apt-key fingerprint 0EBFCD88add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\"apt-get updateapt-get install -yq docker-cecd /rootgit clone https://github.com/codespaces-io/codespaces.gitcurl -L \"https://github.com/docker/compose/releases/download/1.29.1/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-composechmod +x /usr/local/bin/docker-composeNOTE: Credits of the script and content of the course to initcronLater, we have to create a FW rule to allow ssh connections from our IP.Finally, to connect to the vm, we just use ssh and check docker is installed:ssh user@ipWith that, we can confirm that our dev environment was successfully deployed.Installing JenkinsThe objective of installing Jenkins is to use continuous integration. In this case, we’re going to use Helm. Remember that Helm is the package manager of Kubernetes.To install Jenkins, we should follow those steps: add Jenkins repository helm repo add Jenkins [https://charts.Jenkins.io](https://charts.Jenkins.io)helm repo update Create a namespace for CI kubectl create namespace ci Create Jenkins.values.yaml file: controller: serviceType: NodePort resources: requests:cpu: \"400m\"memory: \"512Mi\" limits: cpu: \"2000m\" Install Jenkins helm install -n ci --values Jenkins.values.yaml Jenkins Jenkins/Jenkins When the installation is donde, we have to follow the steps of the output to get the admin password and then login.kubectl exec --namespace ci -it svc/Jenkins -c Jenkins -- /bin/cat /run/secrets/additional/chart-admin-password &amp;&amp; echo[output]t3P4I48zb90j7Dbr3aBC8Jexport NODE_PORT=$(kubectl get --namespace ci -o jsonpath=\"{.spec.ports[0].nodePort}\" services Jenkins)export NODE_IP=$(kubectl get nodes --namespace ci -o jsonpath=\"{.items[0].status.addresses[0].address}\")echo http://$NODE_IP:$NODE_PORT[output]http://10.128.0.6:31893according to the previous output, we have to access this webpage http://10.128.0.6:319893. However, it’s an internal IP and we have to find the external IP adress. To find the external IP, we have to execute this command:kubectl get nodes -o wideIn this case, the url is: http://35.223.160.114:31893/Now, it’s time to configure Jenkins: change admin password. Go to Manage Jenkins → Plugin Manager → available plugins and install: Blue Ocean Configuration as Code Download now and install after restartSetting a simple continuous Integration DevOps pipelineFort his case, we’re going to fork this java repo: https://github.com/lfs262/dso-demo. In this repo, we can see the Jenkins file and see how the pipeline will be:pipeline {agent {Kubernetes {yamlFile 'build-agent.yaml'defaultContainer 'maven'idleMinutes 1}}stages {stage('Build') {parallel {stage('Compile') {steps {container('maven') {sh 'mvn compile'}}}}}stage('Test') {parallel {stage('Unit Tests') {steps {container('maven') {sh 'mvn test'}}}}}stage('Package') {parallel {stage('Create Jarfile') {steps {container('maven') {sh 'mvn package -DskipTests'}}}}}stage('Deploy to Dev') {steps {// TODOsh \"echo done\"}}}}After forking the repo, in Jenkins we have to go to Blue Ocean. Blue Ocean is the UI created to run pipelines.From there, create a new pipeline, link with github account and follow the steps for Jenkins can pull the code, the Jenkins file, create the pipeline, and push status updates to the Git repository on the commit history.Then, Copy the token and paste in JenkinsAfter, create pipeline. This action will read the repository, find the Jenkins file, create the pipeline, and launch it.Also, you can confirm that our Jenkins jobs will be launched as a Kubernetes pods.if you have some error during the CI, it’s because you need to update blue ocean and github plugins.This is a simple Kubernetes native continuous integration pipeline." }, { "title": "Analyzing Packets with Wireshark and Python", "url": "/blog/posts/Analyzing-Packets-with-Wireshark-and-Python/", "categories": "python, wireshark", "tags": "wireshark, security, python", "date": "2022-09-25 03:20:00 +0000", "snippet": "Some cases, as a analysts we need to review network packets to find the root cause of an error or incident. for this reason, in this post we’re going to analyze packets using wireshark and see if w...", "content": "Some cases, as a analysts we need to review network packets to find the root cause of an error or incident. for this reason, in this post we’re going to analyze packets using wireshark and see if we can analyze them with Python.For this post I use this pcap file from a course I have taken called “Bootcamp Analista SOC Nivel 1”. It’s a free and excellent course (taugh in spanish) made by DOJO community.for that cap file we´re going to answer those questions:What protocol did it use the port 3942? With Wireshark:in the filter section we can use this query to see the results:udp.port==3942 || tcp.port ==3942we can observe that in the column called Protocol we see the answer: SSDP. this protocol stands for Simple Service Discovery Protocol. With Python:First, we have to export the pcap file to csv in order to read with pandas module with the use of tshark: !tshark -r 'PCAP01.pcapng' -T fields -E header=y -E separator=, -E quote=d -E occurrence=f -e ip.version -e ip.hdr_len -e ip.tos -e ip.id -e ip.flags -e ip.flags.rb -e ip.flags.df -e ip.flags.mf -e ip.frag_offset -e ip.ttl -e ip.proto -e ip.checksum -e ip.src -e ip.dst -e ip.len -e ip.dsfield -e tcp.port -e tcp.srcport -e tcp.dstport -e tcp.seq -e tcp.ack -e tcp.len -e tcp.hdr_len -e tcp.flags -e tcp.flags.fin -e tcp.flags.syn -e tcp.flags.reset -e tcp.flags.push -e tcp.flags.ack -e tcp.flags.urg -e tcp.flags.cwr -e tcp.window_size -e tcp.checksum -e tcp.urgent_pointer -e tcp.options.mss_val -e udp.port -e _ws.col.Protocol -e _ws.col.Info -e dns.qry.name -e dns.resp.type &gt; output.csvThen, we can read it with:df = pd.read_csv('output.csv',on_bad_lines='skip')In python, the filters are similar to Wireshark but the advantage it’s we can do more with the data.To know what is the name of the protocol with number 3942, we can use this filter:df[(df['tcp.port']==3942) | (df['udp.port']==3942)][['_ws.col.Protocol','_ws.col.Info']]What is the IP address of the host that was pinged twice? With Wireshark:In the filter section we should only write:icmpThen observing the info column which packets begin with request.as a result, the answer is 8.8.4.4 With Python:To filter the ICMP protocol we use:df[(df['_ws.col.Protocol']=='ICMP')][['ip.src','ip.dst','_ws.col.Info']]The table shows us a column called _ws.col.Info where we can see the echo request packets and confirm the destination ip address is 8.8.4.4How many DNS query response packets have been captured? With Wireshark:Here we write in the filter section:dns.resp.typeNext, seeing the bottom of the windows to show the number of packets.The answer is 99 packets Using Python:Using the following filter:df[df['dns.resp.type'].notnull()].count()we confirm that 99 packets is the right answer.What is the source IP address which has sent more packets? Using WiresharkHere, I started to filter with ip address manually. Then, I could observe that 192.168.1.7 has a big part of the percentageAnswering this question with python is better because we can automate with better filters and graph. Using pythonThe groupby method can be used to answer this question.df.groupby(by='ip.src').count().sort_values('_ws.col.Protocol',ascending=False)['_ws.col.Protocol'].head(5).plot.bar()according to the graph, we can see that the ip address 192.168.1.7 has more packets than the rest.If you want to see my jupyter noteebook, you can see here." }, { "title": "Deploying Wazuh in a home lab", "url": "/blog/posts/Deploying-Wazuh-in-a-home-lab/", "categories": "siem, security", "tags": "wazuh, sentinel, siem", "date": "2022-09-04 17:30:00 +0000", "snippet": "In this post we’re going to deploy Wazuh. According to its website, Wazuh is is a free and open source security platform that unifies XDR and SIEM capabilities. It protects workloads across on-pre...", "content": "In this post we’re going to deploy Wazuh. According to its website, Wazuh is is a free and open source security platform that unifies XDR and SIEM capabilities. It protects workloads across on-premises, virtualized, containerized, and cloud-based environments.Wazuh helps organizations and individuals to protect their data assets against security threats. It is widely used by thousands of organizations worldwide, from small businesses to large enterprises.Downloading Wazuh imageIn this link we can download the OVA to deploy Wazuh in Vmware or VirtualBox. After download the OVA, we only have to import it and start it.the credentials to log in are: user: wazuh-user pass: wazuhThen, we should check the ip address to access in a browser with:ip aIn this case is: 192.168.197.139the Wazuh dashboard can be accessed from the web interface by using the following credentials: URL: https:// user: admin pass: adminDeploying agentsin the modules section, we confirm there are no agents enrolled in our Wazuh server and we select add agent to enroll our clients.Next, we should select the options that match with our client where it will be installed the agent. In this scenario, Windows 10 and Ubuntu will be enrolled to Wazuh serverFor Windows 10, those are the options: operating system: Windows Wazuh server address: 192.168.197.139 (ip address of Wazuh server) agent group: default copy the command to execute in Powershell in the client:Invoke-WebRequest -Uri https://packages.wazuh.com/4.x/windows/wazuh-agent-4.3.7-1.msi -OutFile ${env:tmp}\\wazuh-agent-4.3.7.msi; msiexec.exe /i ${env:tmp}\\wazuh-agent-4.3.7.msi /q WAZUH_MANAGER='192.168.197.139' WAZUH_REGISTRATION_SERVER='192.168.197.139' WAZUH_AGENT_GROUP='default'NET START WazuhSvc start wazuh service in windows client with NET START WazuhSvcIn ubuntu, the steps are similar, we only have to change the operating system to Debian / Ubuntu and the command is:curl -so wazuh-agent-4.3.7.deb https://packages.wazuh.com/4.x/apt/pool/main/w/wazuh-agent/wazuh-agent_4.3.7-1_amd64.deb &amp;&amp; sudo WAZUH_MANAGER='192.168.197.139' WAZUH_AGENT_GROUP='default' dpkg -i ./wazuh-agent-4.3.7.debFinally, we start and confirm the wazuh agent is working corretly with:systemctl daemon-reloadsystemctl enable wazuh-agent --nowsystemctl status wazuh-agentExploring DashboardsAccessing from the web interface, we can confirm the agents enrrolled:Finally, we can see the information we cna obtain in Wazuh like security events and results of compliance:" }, { "title": "How to Configure CIS Compliance Report in Nessus", "url": "/blog/posts/Configuring-CIS-Compliance-Report-in-Nessus/", "categories": "security, vulnerability scan", "tags": "nessus, cis compliance, vulnerability scan", "date": "2022-07-23 04:50:00 +0000", "snippet": "Vulnerability scanner is a system designed to assess computers, networks or applications for known weaknesses. This system is a key part in any security program because we can automate those scans ...", "content": "Vulnerability scanner is a system designed to assess computers, networks or applications for known weaknesses. This system is a key part in any security program because we can automate those scans and centralize those results for better analysis in remediation. For that reason, we’re going to install and configure Nessus Manager to check CIS compliance against a Windows Server.Installation Nessus Clic in try in the following web page: https://www.tenable.com/products/nessus to get an email for installation guide Once you have the email, you’ll active the account and in My Trials section Download nessus manager package. In this case, it will be installed in Kali Linux OS update Kali Linuxapt update install Nessus package with the follwing command:dpkg -i Nessus-&lt;version number&gt;-debian6_amd64.deb start and enable Nessus service with those commands: systemctl start nessusd.servicesystemctl enable nessusd.service Configuring nessus.After the Nessus service starts, use a web browser to navigate to the Nessus Web Interface at: https://localhost:8834/.copy and paste the code obtained in the step 2Then, create an username and password. for this case the username is nessus_adm and the pass is 1QazxsW@Next, Nessus will take some minutes to initialize its plugins.Creating a scanNow, that we have all installed, we can create a compliance scan to see how our good or bad is a Windows server according to CIS. For this example, we have another Windows server 2016 and it will be tested with CIS L1. To do that, we can do the following steps: create a compliance scan select target (IP Address) provide credentials select the benchmarkNow, we can save and launch the scan.Creating a custom baseline based on CISit’s important to know, that our server doesn’t have any baseline, it only has all configurations by default. Therefore, the server only has a 84 items passed.However, we can create a custom baseline based on CIS benchmark because an organization can focus on specific items according to the risk that they want to take it. To ilustrate this situation, imagine that our company only has as a baseline a password policy and only wants to see those configurations:. Account lockout duration Enforce password history Interactive logon: Machine account lockout threshold Maximum password age Minimum password age Minimum password length Password must meet complexity requirement Reset lockout counter after Network security: Force logoff when logon hours expireIn this situation, we have to download the CIS bechmark for Windows server 2016 here: https://www.tenable.com/audits/after download the file, we can edit with visual studio or any text editor. the CIS benchmark file looks like this: &lt;custom_item&gt; type : LOCKOUT_POLICY description : \"Account lockout duration\" info : \"Account lockout durationThis security setting determines the number of minutes a locked-out account remains locked out before automatically becoming unlocked. The available range is from 0 minutes through 99,999 minutes. If you set the account lockout duration to 0, the account will be locked out until an administrator explicitly unlocks it.If an account lockout threshold is defined, the account lockout duration must be greater than or equal to the reset time.Default: None, because this policy setting only has meaning when an Account lockout threshold is specified.\" solution : \"Policy Path: Account Policies\\Account Lockout PolicyPolicy Name: Account lockout duration\" reference : \"800-171|3.1.8,800-53|AC-7a.,CN-L3|8.1.4.1(b),CSCv6|16.7,GDPR|32.1.b,HIPAA|164.306(a)(1),ITSG-33|AC-7a.,NESA|T5.5.1,NIAv2|AM24,TBA-FIISB|45.1.2,TBA-FIISB|45.2.1,TBA-FIISB|45.2.2\" see_also : \"https://blogs.technet.microsoft.com/secguide/2016/10/17/security-baseline-for-windows-10-v1607-anniversary-edition-and-windows-server-2016/\" value_type : TIME_MINUTE value_data : [15..MAX] lockout_policy : LOCKOUT_DURATION &lt;/custom_item&gt;after modifying the file, we can edit the scan in Nessus and see the new results.it can be seen that the compliance report is according to the items that we want. Furthermore, if those items are configured, it will show in the next scan as a PASSED status.if we see the history we see that we have improved the compliance report, remediating some items. Before: After Finally, if there is no budget to purchase a vulnerability tool like Nessus, we can explore open source tools like this: https://github.com/0x6d69636b/windows_hardening. Then, we can create a central repository to save all the outputs and finally trying to create a dashboard with the results." }, { "title": "(ESP) Administracion de cuentas locales en el Directorio Activo con LAPS", "url": "/blog/posts/Administracion-de-cuentas-locales-en-el-Directorio-Activo-con-LAPS/", "categories": "security, Active Directory", "tags": "windows, active directory, laps", "date": "2022-05-27 03:20:00 +0000", "snippet": "En ambientes empresariales es dificil realizar una administracion adecuada de las cuentas locales. Por ello, Microsoft tiene una herramienta gratuita llamada LAPS para administrar automaticamente l...", "content": "En ambientes empresariales es dificil realizar una administracion adecuada de las cuentas locales. Por ello, Microsoft tiene una herramienta gratuita llamada LAPS para administrar automaticamente las credenciales de los equipos que estan en el Directorio Activo. LAPS nos asegura que la contraseña de la cuenta local administradora es distinta en cada equipo. Pre-requisitos: SO cliente - Windows Server 2019, Windows Server 2016, Windows 10, Windows Server 2012 R2, Windows Server 2012, Windows Server 2008 R2, Windows Server 2008, Windows Server 2003, Windows 7, Windows 8, Windows Vista, Windows 8.1 Active Directory – Windows Server 2003 SP1 o superior PowerShell 2.0 o superior, .Net Framework 4.0 o superior Tener un ambiente con Directorio Activo. En este caso el dominio se llama: lab.local Cuenta con permisos de Domain Admin Plantillas administrativasPara poder implementar LAPS se debe tener las plantillas administrativas que se pueden descargar aca. En especifico se necesitan la plantilla administrativa llamada AdmPwdPara este tutorial se creará un repositorio centralizado. Esto es recomendable en ambientes donde se tienen mas de un controldo de dominio.ir a la ruta \\\\lab.local\\SYSVOL\\lab.local\\Policies y crear la carpeta PolicyDefinitions copiar el archivo AdmPwd.admx en la carpeta PolicyDefinitions copiart el archivo AdmPwd.adml en la carpeta PolicyDefinitions\\en-USPreparar el ambiente para activar LAPSIdentificar la OU en la que se activará LAPSEn este tutorial, La OU para activar LAPS se llama servers en el que solo tenemos un equipo.Instalar extension de LAPS en los equipos que se requiere administrar la cuenta localde este link se descarga el archivo .msi e iniciamos el que se ajuste a nuestra arquitectura.Para la instalacion solamente necesitamos seguir con las instrucciones que traen por defectoNOTA: Esto tambien se puede desplegar por medio de una politica en el Directorio ActivoActualizar el Schema del Directorio ActivoLAPS usa 2 atributois que por defecto no los tiene: ms-Mcs-AdmPwd : Save the administrator password in clear text ms-Mcs-AdmPwdExpirationTime : Save the timestamp of password expiration.Para extender el schema necesitamos instalar primero LAPS pero habilitando solamente la extension de GPOPosteriormente, se procede a abrir una consola PowerShell en modo administrador y actualizamos el schema de la siguiente mandera:PS C:\\Users\\testing&gt; Import-module AdmPwd.PSPS C:\\Users\\testing&gt; Update-AdmPwdADSchemaCambiar permisos del objeto ComputerDuring the password update process, the computer object itself should have permission to write values to ms-Mcs-AdmPwd and ms-Mcs-AdmPwdExpirationTime attributes. To do that we need to grant permissions to SELF built-in account.To do that, Run command:PS C:\\Users\\testing&gt; Set-AdmPwdComputerSelfPermission -OrgUnit ServersAsignar permisos al grupo para acceso de passwordEn este caso, se dara acceso para leer la contraseña al grupo llamado ITStaff. Para validar quien puede ver las contraseñas ejecutamos:PS C:\\Users\\testing&gt; Find-AdmPwdExtendedRights -Identity serversencontramos que los domain admins son los unicos que por defecto pueden ver las credencialesAhora, adicionamos los privilegios para que el grupo itstaff puedan ver la contraseñaPS C:\\Users\\testing&gt; Set-AdmPwdReadPasswordPermission -Identity \"Servers\" -AllowedPrincipals \"ITStaff\"Crear GPO para LAPSAbrir el Group Policy Management, crear una GPO para activar LAPS bajo la OU de Servers y dar en editar ir a la ruta: Computer Configuration Administrative Templates LAPS Testear LAPSpara acelerar el proceso, se debe actualizar politicas en el servidor con:C:\\Users\\testing&gt;gpupdate /forceEn el controlador de dominio con cuenta domain admin validar sus permisos:si, ensayamos las credenciales entramos con exito:Ahora, si intentamos visualizar el pass con otra cuenta que no tiene acceso (user en este caso) no se ve:y si lo hacemos con una cuenta del grupo ITStaff, comprobamos que el pass se ve:" }, { "title": "Linux Authentication with Active Directory", "url": "/blog/posts/Linux-Authentication-with-Active-Directory/", "categories": "Linux, Active Directory", "tags": "windows, active directory, elastic", "date": "2022-05-24 00:20:00 +0000", "snippet": "This is a tutorial to access with an Active Directory user in a linux server.For this tutorial we have the following devices: Active Directory Server: IP 10.0.0.4 Linux Server: IP 10.0.0.5Instala...", "content": "This is a tutorial to access with an Active Directory user in a linux server.For this tutorial we have the following devices: Active Directory Server: IP 10.0.0.4 Linux Server: IP 10.0.0.5Instalation of Active Directory we need a Windows server and adding the role of Active Directory. Configure the domainIn this case the domain name is: lab.localthen next &gt; install. Afterward, the server will reboot.Check Connectivity Ensure the Linux server (in this case Centos7) responds to the domain.Install requirements in Linux Install the following requirements in order to ensure a proper integration.yum install sssd realmd oddjob oddjob-mkhomedir adcli samba-common samba-common-tools krb5-workstation openldap-clients policycoreutils-python## Join the server to the Active Directory with an anccount of Active Directory execute this command: realm join --user=[userAD] [domain name] check that the server was joined type those commands: realm list, id [userAD]@[domain name] see the server in AD modify sssd configurationmodify the file /etc/sssd/sssd.conf from this:to this:This is used to allow auth without the domain name. restart the service check that you can request info without write user@domain with this command: id [userAD]Auth with an ADUser in the Linux ServerWe can do this by SSH. However, for this example I used only su command.Security ConsiderationAt this point all users from the Active Directory domain can log in the Linux server. However, they do not have sudo privileges. From a security view, we need to ensure that only allowed users can access to the server and not all. To solve that, we need to do the following steps: deny access for all users with this command: realm deny --all permit specific users or groups of Active Directory with those commands: realm permit [user]@domain_name, realm permit -g [groupname]@domain_name restart sssd service" }, { "title": "Monitoring Powershell commands with Elastic Stack", "url": "/blog/posts/Monitoring-Powershell-commands-with-Elastic-Stack/", "categories": "Windows, security", "tags": "windows, siem, elastic", "date": "2022-05-23 01:20:00 +0000", "snippet": "In some cases it’s important to monitor all the powershell commands executed in a windows server because it can help us to alert possible attacks and lateral movements. For that reason, in this pos...", "content": "In some cases it’s important to monitor all the powershell commands executed in a windows server because it can help us to alert possible attacks and lateral movements. For that reason, in this post it will show how to active powershell commands in the event viewer of Windows and then using an agent to send those logs for visualization.Enabling Powershell commands in event viewerWe can enable those settings by using an Active Directory environment or locally in the server. In this example it’s made by AD using the GPO editor: Administrative Templates → Windows Components → Windows PowerShell: Turn on Modle Logging Turn on Powershell Script Block LoggingInstaling agent for monitoring Windows event with Elastic StackIn this case, elastic agent was installed in the windows server. For details about installation click here. Then in kibana you have to create a policy to monitor those Windows channels: Microsoft-Windows-Powershell/Operational channel y Windows Powershell channelTesting that Powershell commands are sent to Elastic StackUse this filter to see Powershell commands in Kibana: event.provider: PowerShell Powershell.command.value: existFurthermore, we can use the dashboard created by Elastic called [Windows powershell] Overview with useful information:" }, { "title": "Visualizing Windows VM access in the Microsoft Sentinel map", "url": "/blog/posts/Visualizing-Windows-VM-access-in-the-Microsoft-Sentinel-map/", "categories": "Azure, security", "tags": "azure, sentinel, siem", "date": "2022-05-12 03:20:00 +0000", "snippet": "In the previous post we deployed Microsoft Sentinel with the connector to gather data from the Windows VM. Now, we can explore some workbooks to see pre-defined dashboardsfor example, we can use th...", "content": "In the previous post we deployed Microsoft Sentinel with the connector to gather data from the Windows VM. Now, we can explore some workbooks to see pre-defined dashboardsfor example, we can use the workbook called “Windows Firewall”. There, it can be seen Windows Security events by account and IP.Furthermore, the workbook Identity &amp; Access shows details about authentications in the VMAlso, we can create queries to see authentication failed attempts. In the Logs sections of the Log Analytics Workspace, we can run:SecurityEvent| where EventID == 4625 However, if we can go the overview tab in Microsoft Sentinel, it doesn’t show in the map the connections of the VM even when those connections are made by public IP adressess.For that reason, we’re going to use a custom script to see conections to the VM in the map. This is an idea from Josh Madakor, thank you for sharing your knowledge.Observing Auth Failed logFirst of all, we need to ensure how to see the log when an authentication failed attempt in the Windows system. To do that, it’s needed to open Event Viewer → Windows Logs folder → Security channelIn the security channel, the event 4625 shows what we want, and it shows important information like: account name source workstation name source IP addressNow, the idea is to take the source IP address with a script which creates a custom log and then it shows the country of the IP in the map with the help of ipgeolocation.io.NOTE: for better results in the SIEM, Windows Firewall is disabled. However, in a production envirtonment, you should keep turn it on.Using the Scriptto begin with, we have to download the script here. Then, we must update the API_KEY varibale for using the script.In order to get the API key, we have to create an account in ipgeolocation.io. Next, go to Dashboard and you see the API Key.The script gathers all the information of failed authentications, it takes the IP address and creates a custom log.Now, it’s time to run the script and see its results:As you can see, there are real failed authentication attempts from Rusia.Creating custom log in Log Analytics Workspace to bring the log from the scriptFirstly, in the azure portal, we go to the Log Analytics workspace → custom log → add custom logSecond, a copy of the log file created by the script must be downloaded in our local computer because it should be uploaded in the azure portal.The log has the following format:latitude:47.91542,longitude:-120.60306,destinationhost:samplehost,username:fakeuser,sourcehost:24.16.97.222,state:Washington,country:United States,label:United States - 24.16.97.222,timestamp:2021-10-26 03:28:29latitude:-22.90906,longitude:-47.06455,destinationhost:samplehost,username:lnwbaq,sourcehost:20.195.228.49,state:Sao Paulo,country:Brazil,label:Brazil - 20.195.228.49,timestamp:2021-10-26 05:46:20latitude:52.37022,longitude:4.89517,destinationhost:samplehost,username:CSNYDER,sourcehost:89.248.165.74,state:North Holland,country:Netherlands,label:Netherlands - 89.248.165.74,timestamp:2021-10-26 06:12:56latitude:40.71455,longitude:-74.00714,destinationhost:samplehost,username:ADMINISTRATOR,sourcehost:72.45.247.latitude:55.88802,longitude:37.65136,destinationhost:vm-sw,username:Mel,sourcehost:94.232.44.12,state:Central Federal District, country:Russia,label:Russia - 94.232.44.12,timestamp:2022-05-10 00:16:26latitude:55.88802,longitude:37.65136,destinationhost:vm-sw,username:db2admin,sourcehost:94.232.44.12,state:Central Federal District, country:Russia,label:Russia - 94.232.44.12,timestamp:2022-05-10 00:16:24latitude:55.88802,longitude:37.65136,destinationhost:vm-sw,username:Administrator,sourcehost:94.232.44.12,state:Central Federal District, country:Russia,label:Russia - 94.232.44.12,timestamp:2022-05-10 00:16:22Third, record delimiter → new line &amp; → NextNext, in the collection paths section selects: Type: Windows Path (path of the log): C:\\ProgramData\\failed_rdp.logAfter, in the details section, we put a name for the custom log: FAILED_RDP_WITH_GEO and then create.After aprox 20 minutes, we can query those logs in the Log analytics Workspace:Nevertheless, those logs don’t have their fields created. Therefore, we have to parse the log with the following: select a log → right clic → Extract field from …Notice the log has this format:field1:value,field2:value,field3:value...according to the log format, we have to select each value and save it in a field name. for example, with latitude field we have to save the extraction:For the rest of the field, we have to do the same process: select a log → right clic → Extract field from … → select the value of the field → give field name → select field type → extract → save extraction.After some minutes, if you run again the query, you’ll see the new fields in the results sections for the new logs:NOTE: in case that some of your field values in the search results is not highlighted, you have to modify that registry to correct its parsing.Besides, in the Custom logs → Custom fields, we can see the field names we created previuosly. In case that one of those fields is presenting a wrong parsing, we have to delete the custom field and extract it again.Setup map in sentinel with latitude and longitude (or country)Now, in the Microsoft Sentinel section, we visit Workbooks → Add Workbook. Then, in edition mode, removing all the widgets.Afterward, add → add querypaste this query:FAILED_RDP_WITH_GEO_CL | summarize event_count=count() by sourcehost_CF, latitude_CF, longitude_CF, country_CF, label_CF, destinationhost_CF| where destinationhost_CF != \"samplehost\"| where sourcehost_CF != \"\"the query shows real failed authentication attempts by countsIn order to create the map, we choose map as visualization and the map settings will be shown to configure properly the map: locations info using: we can choose latitude/longitude or country/region. For this example, we’re using country/region Country/Region field: country_cf size by: event_count Metric value: event_countFinally, apply to update the map and save and closeThen, save the map, put a title: Failed RDP map, set the location, and save it.At this moment, we have to wait some minutes or hours to see new failed authentications and see in the map." }, { "title": "Deployment of Microsoft Sentinel (SIEM)", "url": "/blog/posts/Deployment-Microsoft-Sentinel-(SIEM)/", "categories": "Azure, security", "tags": "azure, sentinel, siem", "date": "2022-04-28 01:20:00 +0000", "snippet": "Creating Azure Subscription We need to visit Azure web site. There, you should click in “Start Free” and continue with the steps.Now, we can sign in portal.azure.com.Creating VM in Azure In order ...", "content": "Creating Azure Subscription We need to visit Azure web site. There, you should click in “Start Free” and continue with the steps.Now, we can sign in portal.azure.com.Creating VM in Azure In order to create our VM, we click on “Virtual machines” option. Next, click in “Create” → “Azure virtual machine” with the following parameters: Resource group: create new → “SentinelLab” virtual machine name: server-vm Region: Central US user and passThis server will serve as a vulnerable machine. Therefore, In the networking section, we are going to create a NSG (Network Security Group) which allows all traffic. The configurations is this: NIC network security group: advanced configure network security group: create new → create inbound rule: from any to any by all protocols with priority 100NOTE: You shouldn’t do this in a real enrivonment.Finally, click in “review+create”when the VM is deployed, we should test our RDP. To do that, we can run “mstsc” and test our connection.Creating a log analytics workspace Now the goal is to ingest logs from the virtual machine. For that, it needs to create a log analytics workspace. here is a description of what log analytic is:Log Analytics collects data from a variety of sources and uses a powerful query language to give you insights into the operation of your applications and resources. Use Azure Monitor to access the complete set of tools for monitoring all of your Azure resources.After, in the search bar we type log analytics workspaces → create log analytics workspace.for this example, we select those parameters: resource group: the same of the VM name: LAW-Sentinel region: Central USFinally, Sentinel will connect to this workspace in order to display the dataEnablig Microsoft Defender for CloudNow, it’s time to enable gather logs from VM into log analytics workspace. So, you need to search it and go to environment settings → select the subscription → auto provisioning → Log Analytics agent for Azure VMs to on → connect Azure VMs to a differente workspace: LAW-SentinelAfterward, we should back to the log analytics workspace to connect the VMNote: the VM must be running to connect with the log analytics workspaceSet up Sentinel type Microsoft Sentinel in the search bar → create Microsoft Sentinel → select the workspacewe have a 30-day trial:Microsoft Sentinel free trial activated:The free trial is active on this workspace from 26/4/2022 to 27/5/2022 at 23:59:59 UTC.During the trial, up to 10 GB/day are free for both Microsoft Sentinel and Log Analytics. Data beyond the 10 GB/day included quantity will be billedFinally, we did a simple set up of Microsoft Sentinel and we can explore all features can give us Sentinel." } ]
